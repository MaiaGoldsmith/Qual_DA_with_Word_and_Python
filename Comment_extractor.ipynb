{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47592e2",
   "metadata": {},
   "source": [
    "## Qualitative data analysis with Microsoft Word comments & Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3361c",
   "metadata": {},
   "source": [
    "#####  Instructions based on article - https://carstenknoch.com/2020/02/qualitative-data-analysis-with-microsoft-word-comments-python-updated/\n",
    "###### Given a .docx file, extract a CSV list of all tagged (commented) text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "# Install the Beautiful Soup library.\n",
    "!pip install beautifulsoup4\n",
    "\n",
    "# Install lxml\n",
    "!pip install lxml\n",
    "\n",
    "# Import the requests and Beautiful Soup libraries.\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import csv\n",
    "import re\n",
    "from io import BytesIO\n",
    "from IPython.display import display, FileLink\n",
    "from ipywidgets import FileUpload\n",
    "import os\n",
    "\n",
    "# Optional - Ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a .docx file, extract a CSV list of all tagged (commented) text\n",
    "# Updated code using chatgpt to troubleshoot\n",
    "\n",
    "\n",
    "# Step 1: Upload files using Jupyter's file upload widget\n",
    "# Files must be closed in order to select them.\n",
    "upload = FileUpload(accept=\".docx\", multiple=True)\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries outside the loop to store data from all files\n",
    "all_start_loc = {}\n",
    "all_end_loc = {}\n",
    "\n",
    "if upload.value:\n",
    "    for filename, file_info in upload.value.items():\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        content = BytesIO(file_info['content'])\n",
    "\n",
    "        try:\n",
    "            unzip = zipfile.ZipFile(content)\n",
    "\n",
    "            # Extract comments.xml\n",
    "            comments = BeautifulSoup(unzip.read('word/comments.xml'), 'lxml')\n",
    "\n",
    "            # Extract document.xml\n",
    "            doc = unzip.read('word/document.xml').decode()\n",
    "\n",
    "            # Find comment start and end locations\n",
    "            start_loc = {x.group(1): x.start() for x in re.finditer(r'<w:commentRangeStart.*?w:id=\"(.*?)\"', doc)}\n",
    "            end_loc = {x.group(1): x.end() for x in re.finditer(r'<w:commentRangeEnd.*?w:id=\"(.*?)\".*?>', doc)}\n",
    "\n",
    "            # Update the dictionaries for all files\n",
    "            all_start_loc.update(start_loc)\n",
    "            all_end_loc.update(end_loc)\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Error: {filename} is not a valid zip file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path of the directory\n",
    "directory_path = os.path.abspath('/'.join(filename.split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV using the dictionaries\n",
    "csv_path = os.path.join(directory_path, 'responses.csv')\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    csvw = csv.writer(csvfile)\n",
    "\n",
    "    # Loop through comments using all_start_loc and all_end_loc\n",
    "    for c_id, start in all_start_loc.items():\n",
    "        end = all_end_loc[c_id]\n",
    "\n",
    "        xml = re.sub(r'(<w:p .*?>)', r'\\1 ', doc[start:end + 1])\n",
    "        label_text = ''.join(BeautifulSoup(xml, 'lxml').findAll(text=True))\n",
    "        comment_text = ''.join(comments.find(attrs={'w:id': c_id}).findAll(text=True))\n",
    "\n",
    "        # Write to CSV\n",
    "        csvw.writerow([comment_text, label_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fc3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
